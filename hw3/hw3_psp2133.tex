\documentclass[fleqn]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{fontspec}
\usepackage{url}

\title{Machine Learning - Homework 3}
\author{Parita Pooj (psp2133)}
\date{October 12, 2016}

\newcommand\tab[1][0.6cm]{\hspace*{#1}}

\begin{document}
\maketitle
\setcounter{secnumdepth}{0}
\section{Problem 1}{}
\begin{itemize}
        \item[(a)]
		\textbf{Centering}: No, the transformation does not affect the learning algorithm.
		Centering will basically make the mean $\bm{\hat{\mu}}$. This can be shown as below:\\
		Let $\bm{\hat{\mu}'}$ be the new mean parameter trained on the transformed data.
		$ \bm{\hat{\mu}'} = \frac{1}{n} \smashoperator{\sum_{i = 0}^{n}} (\bm{x} - \bm{\hat{\mu}})$ \\
		$ \bm{\hat{\mu}'} = \frac{1}{n} \smashoperator{\sum_{i = 0}^{n}} \bm{x} 
		                  - \frac{1}{n} \smashoperator{\sum_{i = 0}^{n}} \bm{\hat{\mu}})$ \\
		$ \bm{\hat{\mu}'} = \bm{\hat{\mu}} - \bm{\hat{\mu}} $ \\
		$ \bm{\hat{\mu}'} = 0 $\\
		Thus, centering essentially transforms the mean to zero, but the distribution still remains the same and hence, the classification won't be affected.\\
		\\
		\textbf{Standardization}: No, standardization does not affect the learning algorithm.
		Standardization makes the standard deviation 1 for each feature, thus not affecting the classification, same as above.\\
        \item[(b)]
		\textbf{Centering}: No, Centering preserves the order of the Euclidean distance between every pair of points.
		Hence, 1-NN classifier will not be affected.\\
		The preservation of the order of distances can be shown by considering three points $\bm{x_p}$, $\bm{x_q}$ and $\bm{x_r}$ such that:\\
		$ \smashoperator{\sum_{i = 1}^{n}} (x_{p,i} - x_{q, i})^2 \leq \smashoperator{\sum_{i = 1}^{n}} (x_{p,i} - x_{r, i})^2 $\\
		For transformed points, $\bm{x_p'}$, $\bm{x_q'}$ and $\bm{x_r'}$ we see that \\
		$ \smashoperator{\sum_{i = 1}^{n}} (x_{p,i}' - x_{q, i}')^2 \leq \smashoperator{\sum_{i = 1}^{n}} (x_{p,i}' - x_{r, i}')^2 $\\
		since, \\
		$ \smashoperator{\sum_{i = 1}^{n}} (x_{p,i} - \hat{\mu}_i - x_{q, i} + \hat{\mu}_i)^2 
			\leq \smashoperator{\sum_{i = 1}^{n}} (x_{p,i} - \hat{\mu}_i - x_{r, i} + \hat{\mu}_i)^2$ \\

		\textbf{Standardization}: No, standardization also doesn't affect the learning algorithm since it preserves the order.
		As above, for transformed points, $\bm{x_p'}$, $\bm{x_q'}$ and $\bm{x_r'}$ we see that \\
		$ \smashoperator{\sum_{i = 1}^{n}} (x_{p,i}' - x_{q, i}')^2 \leq \smashoperator{\sum_{i = 1}^{n}} (x_{p,i}' - x_{r, i}')^2 $\\
		since, \\
		$ \smashoperator{\sum_{i = 1}^{n}} (\frac{x_{p,i} - \hat{\mu}_i - x_{q, i} + \hat{\mu}_i}{\sigma_i})^2 
			\leq \smashoperator{\sum_{i = 1}^{n}} (\frac{x_{p,i} - \hat{\mu}_i - x_{r, i} + \hat{\mu}_i}{\sigma_i})^2$ \\


        \item[(c)]
		\textbf{Centering}: No, the transformation does not affect the learning algorithm.
		The Gini Index uncertainty measure considers the probability distribution of the points with respect to the axis split.\\
		Centering essentially doesn't affect the algorithm because the axis split undergoes an equivalend transformation, 
		preserving the distribution with respect to the split. Due to this, the classification still remains the same.\\
		This can be shown with a simple example for feature $i$. Let $x_i > c$ be one of the axis splits, 
		which classifies some data points $x_p$, $x_q$, $x_r$, $x_s$ as below:\\
		$x_{p, i} < x_{q, i} < c < x_{r, i} < x_{s, i}$ for feature $i$\\
		For the transformed data, $x_i - \mu_i > c$ or $x_i > c + \mu_i$ corresponds to the transformed axis 
		which preserves the classification since it preserves the inequality.

		\textbf{Standardization}: No, the transformation does not affect the learning algorithm. 
		From above, we can see that dividing by $\sigma_i$ preserves the inequality.
		Thus, the distribution for classification remains the same since the particular axis split 
		will give the same Gini Index uncertainty measure which corresponds to the maximally reduced uncertainty.

        \item[(d)]
\end{itemize}
\section{Problem 2}{}
\begin{itemize}
	\item[(a)] 
        \item[(b)] 
\end{itemize}
\section{Problem 3}{}
\begin{itemize}
        \item[(a)]
		The multivariate Gaussian distribution can be written as:
		\[
			P_{\mu, \sigma^2} = \prod_{i = 1}^{n} \frac{1}{(2 \pi)^{d/2} |\Sigma|^{1/2}} e^{\frac{-1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) }
		\]
		where $\Sigma = \sigma^2 I$
		\[
			\ln P_{\mu, \sigma^2} = 
			\smashoperator{\sum_{i = 1}^{n}} 
				\frac{-1}{2} \ln(2\pi) 
				- \frac{1}{2} \ln(|\Sigma|) 
				- \frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu})
		\]
		\[
			\ln P_{\mu, \sigma^2} = 
			- \frac{1}{2} \smashoperator{\sum_{i = 1}^{n}} 
				\ln(2\pi) 
				+ \ln(|\Sigma|) 
				+ (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu})
		\]
		By matrix derivation rules[1],
		\[
			\frac{\partial \ln P_{\mu, \sigma^2}}{\partial \Sigma} = 
			- \frac{1}{2} \smashoperator{\sum_{i = 1}^{n}} 
				0 + |\Sigma|^{-T} 
				+ (- \Sigma^{-T}(\bm{x} - \bm{\mu}) (\bm{x} - \bm{\mu}) \Sigma^{-T})
		\]
		\[
			\frac{\partial \ln P_{\mu, \sigma^2}}{\partial \Sigma} = 0
		\]
		\[
			- \frac{1}{2} \smashoperator{\sum_{i = 1}^{n}} 
			[ |\Sigma|^{-T} 
			- \Sigma^{-T}(\bm{x} - \bm{\mu}) (\bm{x} - \bm{\mu})^T \Sigma^{-T} ]
			= 0
		\]
		Since, $\Sigma$ is a diagonal matrix: $\Sigma^{-T} = \Sigma^{-1}$
		\[
			\smashoperator{\sum_{i = 1}^{n}} 
			[ |\Sigma|^{-1} 
			- \Sigma^{-1}(\bm{x} - \bm{\mu}) (\bm{x} - \bm{\mu})^T \Sigma^{-1} ]
			= 0
		\]
		\[
			\smashoperator{\sum_{i = 1}^{n}} |\Sigma|^{-1} =
			\smashoperator{\sum_{i = 1}^{n}} 
			[ \Sigma^{-1}(\bm{x} - \bm{\mu}) (\bm{x} - \bm{\mu})^T \Sigma^{-1} ]
		\]
		\[
			\smashoperator{\sum_{i = 1}^{n}} I =
			\smashoperator{\sum_{i = 1}^{n}} 
			[ (\bm{x} - \bm{\mu}) (\bm{x} - \bm{\mu})^T \Sigma^{-1} ]
		\]
		\[
			nI =
			\smashoperator{\sum_{i = 1}^{n}} [ (\bm{x} - \bm{\mu}) (\bm{x} - \bm{\mu})^T] \Sigma^{-1}
		\]
		\[
			n\Sigma =
			\smashoperator{\sum_{i = 1}^{n}} [ (\bm{x} - \bm{\mu}) (\bm{x} - \bm{\mu})^T ]
		\]
		\[
			\Sigma = \frac{1}{n} \smashoperator{\sum_{i = 1}^{n}} [ (\bm{x} - \bm{\mu}) (\bm{x} - \bm{\mu})^T ]
		\]
		\[
			\sigma^2 I = \frac{1}{n} \smashoperator{\sum_{i = 1}^{n}} [ (\bm{x} - \bm{\mu}) (\bm{x} - \bm{\mu})^T ]
		\]
 		\[
			\sigma^2 = \frac{1}{n} \smashoperator{\sum_{i = 1}^{n}} [\smashoperator{\sum_{j = 1}^{d}} (\bm{x_j} - \bm{\mu_j})^2]
		\]
        \item[(b)]
\end{itemize}

\begin{thebibliography}{99}
        \bibitem{[1]} \url{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}
        \bibitem{[2]} \url{http://cs229.stanford.edu/section/gaussians.pdf}

\end{thebibliography}
\end{document}
